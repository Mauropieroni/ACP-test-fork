{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "print(\"WARNING: CUSTOM VERSION OF SBI REQUIRED\")\n",
    "print(\"This can be found at: https://github.com/james-alvey-42/sbi-acp\")\n",
    "import matplotlib.pyplot as plt\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 3\n",
    "prior = utils.BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n",
    "\n",
    "def simulator(theta):\n",
    "    # linear gaussian\n",
    "    return theta + 1.0 + torch.randn_like(theta) * 0.1\n",
    "\n",
    "# Check prior, simulator, consistency\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator, prior)\n",
    "theta_obs = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "x_obs = simulator(theta_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R(samples):\n",
    "    \"\"\"\n",
    "    Computes the Gelman-Rubin (GR) statistic for convergence assessment. The\n",
    "    GR statistic is a convergence diagnostic used to assess whether multiple\n",
    "    Markov chains have converged to the same distribution. Values close to 1\n",
    "    indicate convergence. For details see\n",
    "    https://en.wikipedia.org/wiki/Gelman-Rubin_statistic\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : numpy.ndarray\n",
    "        Array containing MCMC samples with dimensions\n",
    "        (N_steps, N_chains, N_parameters).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    R : numpy.ndarray\n",
    "        Array containing the Gelman-Rubin statistics indicating convergence for\n",
    "        the different parameters. Values close to 1 indicate convergence.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shapes\n",
    "    N_steps, N_chains, N_parameters = samples.shape\n",
    "\n",
    "    # Chain means\n",
    "    chain_mean = np.mean(samples, axis=0)\n",
    "\n",
    "    # Global mean\n",
    "    global_mean = np.mean(chain_mean, axis=0)\n",
    "\n",
    "    # Variance between the chain means\n",
    "    variance_of_means = (\n",
    "        N_steps\n",
    "        / (N_chains - 1)\n",
    "        * np.sum((chain_mean - global_mean[None, :]) ** 2, axis=0)\n",
    "    )\n",
    "\n",
    "    # Variance of the individual chain across all chains\n",
    "    intra_chain_variance = np.std(samples, axis=0, ddof=1) ** 2\n",
    "\n",
    "    # And its averaged value over the chains\n",
    "    mean_intra_chain_variance = np.mean(intra_chain_variance, axis=0)\n",
    "\n",
    "    # First term\n",
    "    term_1 = (N_steps - 1) / N_steps\n",
    "\n",
    "    # Second term\n",
    "    term_2 = variance_of_means / mean_intra_chain_variance / N_steps\n",
    "\n",
    "    # This is the R (as a vector running on the paramters)\n",
    "    return term_1 + term_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 10\n",
    "models = {}\n",
    "for idx in range(num_models):\n",
    "    inference = NPE(prior=prior)\n",
    "\n",
    "    # generate simulations and pass to the inference object\n",
    "    theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=2000)\n",
    "    inference = inference.append_simulations(theta, x)\n",
    "    models[\"model_\" + str(idx)] = {\n",
    "        \"inference\": inference,\n",
    "        \"posterior_samples\": [],\n",
    "        \"theta\": theta,\n",
    "        \"x\": x,\n",
    "        \"epochs_trained\": 0,\n",
    "        \"train_losses\": [],\n",
    "        \"validation_losses\": [],\n",
    "        \"state_dict_loc\": \"state_dict/model_\" + str(idx) + \"_epoch_\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "for epoch in range(max_epochs):\n",
    "    for model in models:\n",
    "        model_info = models[model]\n",
    "        _density_estimator = model_info[\"inference\"].train(\n",
    "            max_num_epochs=0,\n",
    "            # learning_rate=1e-4,\n",
    "            model_info=model_info,\n",
    "            force_first_round_loss=True,\n",
    "        )\n",
    "        _posterior = model_info[\"inference\"].build_posterior(_density_estimator)\n",
    "        _posterior_samples = _posterior.sample((1000,), x=x_obs)\n",
    "        model_info[\"posterior_samples\"].append(_posterior_samples)\n",
    "        model_info[\"epochs_trained\"] += 1\n",
    "\n",
    "for model in models:\n",
    "    model_info = models[model]\n",
    "    model_info[\"train_losses\"] = model_info[\"inference\"]._summary[\"training_loss\"]\n",
    "    model_info[\"validation_losses\"] = model_info[\"inference\"]._summary[\"validation_loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for model in models:\n",
    "    model_info = models[model]\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_info[\"train_losses\"])\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(model_info[\"validation_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.viridis(np.linspace(0, 1, num_models))\n",
    "# make a gif over the epochs\n",
    "for epoch in range(max_epochs):\n",
    "    for m_idx, model in enumerate(models):\n",
    "        model_info = models[model]\n",
    "        if m_idx == 0:\n",
    "            corner.corner(\n",
    "                model_info[\"posterior_samples\"][epoch].numpy(),\n",
    "                labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n",
    "                truths=[1.0, 1.0, 1.0],\n",
    "                color=colors[m_idx],\n",
    "            )\n",
    "        else:\n",
    "            corner.corner(\n",
    "                model_info[\"posterior_samples\"][epoch].numpy(),\n",
    "                fig=plt.gcf(),\n",
    "                color=colors[m_idx],\n",
    "            )\n",
    "    plt.savefig(\n",
    "        \"../figures/epoch_\" + str(epoch) + \".png\"\n",
    "    )\n",
    "    plt.clf()\n",
    "\n",
    "import imageio\n",
    "\n",
    "images = []\n",
    "for epoch in range(max_epochs):\n",
    "    images.append(imageio.imread(f\"figures/epoch_{epoch}.png\"))\n",
    "imageio.mimsave('../figures/posterior.gif', images, duration=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_values = []\n",
    "for epoch in range(max_epochs):\n",
    "    post_samples = []\n",
    "    for model in models:\n",
    "        model_info = models[model]\n",
    "        post_samples.append(model_info[\"posterior_samples\"][epoch].numpy())\n",
    "    R_val = get_R(np.transpose(post_samples, (1, 0, 2)))\n",
    "    R_values.append(R_val)\n",
    "plt.plot(np.array(R_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for model in models:\n",
    "    model_info = models[model]\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_info[\"train_losses\"])\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(model_info[\"validation_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_values = []\n",
    "for epoch in range(max_epochs):\n",
    "    post_samples = []\n",
    "    for _ in range(10):\n",
    "        post_samples.append(np.random.randn(1000, 3))\n",
    "    R_val = get_R(np.transpose(post_samples, (1, 0, 2)))\n",
    "    R_values.append(R_val)\n",
    "plt.plot(np.array(R_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_values = []\n",
    "for epoch in range(max_epochs):\n",
    "    post_samples = []\n",
    "    for _ in range(10):\n",
    "        post_samples.append(np.random.randn(100, 3))\n",
    "    R_val = get_R(np.transpose(post_samples, (1, 0, 2)))\n",
    "    R_values.append(R_val)\n",
    "plt.plot(np.array(R_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_values = []\n",
    "for epoch in range(max_epochs):\n",
    "    post_samples = []\n",
    "    for _ in range(10):\n",
    "        post_samples.append(np.random.randn(10, 3))\n",
    "    R_val = get_R(np.transpose(post_samples, (1, 0, 2)))\n",
    "    R_values.append(R_val)\n",
    "plt.plot(np.array(R_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = []\n",
    "true_posterior = np.random.normal(x_obs - 1, 0.1, (10000, 3))\n",
    "for model in models:\n",
    "    model_info = models[model]\n",
    "    post_samples.append(model_info[\"posterior_samples\"][-1].numpy())\n",
    "print(np.array(post_samples).shape)\n",
    "corner.corner(np.reshape(np.array(post_samples), (10 * 1000, 3)));\n",
    "#corner.corner(model_info[\"posterior_samples\"][-1].numpy(), color='green', fig=plt.gcf())\n",
    "corner.corner(true_posterior, fig = plt.gcf(), color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
